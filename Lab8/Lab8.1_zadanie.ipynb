{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa8d1ce",
   "metadata": {},
   "source": [
    "## Laboratorium 8.1\n",
    "\n",
    "\n",
    "## Metody wykrywania ruchu: przepływ optyczny (_optical flow_)\n",
    "\n",
    "### Wstęp\n",
    "\n",
    "Na poprzednich laboratoriach poznaliśmy podstawowe metody przetwarzania obrazów, a więc dwuwymiarowych sygnałów przestrzennych. Jednak w praktyce czasami dysponujemy materiałem wideo, a więc sygnałami _trójwymiarowymi_, gdzie trzecim wymiarem jest czas. Oczywiście, można takie dane traktować jako po prostu sekwencję niezależnych obrazów - i wtedy działają wszystkie poznane dotychczas metody. Jednak dlaczego by nie wykorzystać tej dziedziny czasowej do przetwarzania sygnału? Intuicyjnie czujemy, że jeśli jakiś rejon obrazu przesunął się w czasie, to prawdopodobnie ma on inne _znaczenie_ niż rejon, który pozostał w tym samym miejscu, lub przesunął w innym kierunku czy z inną szybkością. Jeśli zatem jesteśmy w stanie wykryć to przesunięcie - czyli po prostu _ruch_ - to otrzymamy pewną informację o zawartości obrazu (wideo). Na przykład, będziemy w stanie oddzielić poruszające się obiekty od stacjonarnego tła, albo oddzielić inaczej poruszające się obiekty.\n",
    "\n",
    "Najpowszechniejszą metodą do automatycznej detekcji ruchu jest metoda przepływu optycznego (ang. _optical flow_). Pominiemy w tej instrukcji matematyczne podstawy tej metody - te są wystarczająco dobrze wyłożone w [artykule z dokumentacji OpenCV](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html), do którego lektury namawiam\\*. Wiedzieć należy na pewno, że istnieją dwie główne odmiany metody przepływu optycznego:,\n",
    "* przepływ gęsty (_dense_) - gdzie przesunięcie pomiędzy klatkami określane jest dla każdego piksela (na tej metodzie skupia się niniejsza część listy),\n",
    "* przepływ rzadki (_sparse_) - gdzie ruch wykrywany jest tylko dla pewnego zbioru punktów zainteresowania w obrębie obrazu (przebadasz ją w drugiej części listy).\n",
    "\n",
    "W ramach obu odmian występuje pewna liczba konkretnych metod obliczeniowych, w zależności od konkretnego podejścia do rozwiązywania równania ruchu. Na tych zajęciach wykorzystamy [algorytm Farnebacka](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf) - głównie dlatego, że jego gotowa implementacja znajduje się w pakiecie OpenCV.\n",
    "\n",
    "\\* - Czytając, zwróć uwagę na podział na sekcje _Lucas-Kanade_ oraz _Dense Optical Flow_. Analizując kod metody, poświęć chwilę na zrozumienie mapowania wyników do prezentowanego obrazu w przestrzeni HSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c9da1",
   "metadata": {},
   "source": [
    "### Podejście\n",
    "\n",
    "Większość algorytmów optical flow operuje na parze klatek, znajdując translację pomiędzy jedną a drugą. Jeśli więc interesuje nas przetwarzanie ciągłego strumienia wideo, praca przebiegać będzie na zasadzie dwuelementowej kolejki, tzn. zawsze patrzymy na klatkę obecną i poprzednią.\n",
    "\n",
    "OpenCV oferuje banalnie prosty a zarazem potężny interfejs do obsługi strumieni wideo: [`cv2.VideoCapture`](https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html), za pomocą którego w ten sam sposób możemy obsługiwać pliki wideo w różnym kodowaniu, urządzenia wideo (np. kamerkę w laptopie) czy nawet wideo w protokole IP (choć występują pewne różnice z punktu widzenia użycia, jeśli korzystamy z zasobu hardware'owego działającego w czasie rzeczywistym). Idea jest prosta:\n",
    "* tworzymy obiekt `cv2.VideoCapture` w odpowiedni sposób,\n",
    "* pobieramy poszczególne klatki za pomocą metody [`VideoCapture::read`](https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html#a473055e77dd7faa4d26d686226b292c1).\n",
    "\n",
    "Metoda `read` wykonuje całą pracę (odczytanie danych, dekodowanie strumienia wideo) i zwraca klatkę jako obraz w standardowym formacie OpenCV (a także flagę, czy w ogóle udało się pozyskać dane - krotka (flaga, klatka)). Zatem, aby pozyskać pierwszą klatkę z pliku wideo wystarczy:\n",
    "```python\n",
    "vid = cv2.VideoCapture(\"back.mp4\")\n",
    "r, frame = vid.read()\n",
    "```\n",
    "\n",
    "Drobnym ograniczeniem VideoCapture jest to, że nie ma możliwości cofnięcia się do poprzednio pobranej klatki (co jest naturalne w przypadku korzystania z fizycznego urządzenia do akwizycji, a może trochę mniej gdy czytamy z pliku). Jeśli potrzebny jest powrót do początku pliku wideo, niestety konieczne jest zamknięcie strumienia (`VideoCapture::release`) i ponowne otwarcie (`::open`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4ae50",
   "metadata": {},
   "source": [
    "Aby obliczyć (gęsty) przepływ optyczny pomiędzy dwiema klatkami, wykorzystamy algorytm Farnebacka, zaimplementowany w OpenCV w funkcji [`cv2.calcOpticalFlowFarneback`](https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga5d10ebbd59fe09c5f650289ec0ece5af). Przyjmuje ona parę obrazów **w skali szarości**, opcjonalny argument `flow` (rozwiązanie można zainicjować poprzednio wyliczonym przepływem, jeśli nim dysponujemy), a następnie szereg parametrów sterujących metodą; m.in. można wykorzystać piramidyzację obrazów (rekomendowane `pyr_size` $=3$) czy określić rozmiar okna detekcji `winsize`. Sensowne pierwsze wartości dla argumentów podane są w dokumentacji.\n",
    "\n",
    "Algorytm Farnebacka zwraca przepływ w formie obrazu o wymiarach przestrzennych równych obrazom wejściowym i dwóch kanałach, zawierających przesunięcie odpowiednio w osi $x$ i $y$. Można więc przetwarzać te dane dalej, np. obliczając kąt przesunięcia czy całkowitą odległość (i dalej, np. określić prędkość ruchu) - vide np. `cv2.cartToPolar`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5d444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from qwlist import Lazy\n",
    "from typing import Iterable\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc12bc95",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "\n",
    "#### Zadanie 1a\n",
    "Otwórz wideo `kick.mp4` lub `back.mp4` i pobierz kilka klatek. Przewiń do interesującego Cię momentu - tak, aby uzyskać dwie klatki, na których widać ruch (pro-tip: znając framerate materiału (~25fps) i czas, w którym rozpoczyna się interesujący fragment, możesz w pętli \"skonsumować\" odpowiednią ilość klatek).  \n",
    "Wykorzystaj algorytm Farnebacka do obliczenia przepływu pomiędzy klatkami. Wynik zaprezentuj w postaci obrazu całkowitego przesunięcia. W zależności od wybranego momentu w wideo, możesz spodziewać się uzyskania wyraźnych obszarów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b2a34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(video: cv2.VideoCapture):\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count / fps\n",
    "    return fps, frame_count, duration\n",
    "\n",
    "def frame_pair_iter(path: str) -> Lazy[tuple[np.ndarray, np.ndarray]]:\n",
    "    def inner_generator() -> Iterable[tuple[np.ndarray, np.ndarray]]:\n",
    "        video1: cv2.VideoCapture = cv2.VideoCapture(path)\n",
    "        video2: cv2.VideoCapture = cv2.VideoCapture(path)\n",
    "        success, _ = video2.read()\n",
    "        while success:\n",
    "            _, frame1 = video1.read()\n",
    "            success, frame2 = video2.read()\n",
    "            if success:\n",
    "                yield frame1, frame2\n",
    "        video1.release()\n",
    "        video2.release()\n",
    "    return Lazy(inner_generator())\n",
    "\n",
    "def make_gif(images: list[Image.Image], path: str, duration=200):\n",
    "    images[0].save(\n",
    "        path,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=duration,\n",
    "        loop=0\n",
    "    )\n",
    "\n",
    "def ferneback(video_path: str, save_dir: str, skip: int = 0, duration: int = 200, winsize: int = 15):\n",
    "    file_name = os.path.basename(video_path).split('.')[0]\n",
    "    _, frame_count, _ = get_video_details(cv2.VideoCapture(video_path))\n",
    "    cmap = LinearSegmentedColormap.from_list('custom', ['#FFE5E5', '#E0AED0', '#AC87C5', '#756AB6'])\n",
    "    gen = frame_pair_iter(video_path).skip(skip)\n",
    "    flow_frames_x = []\n",
    "    flow_frames_y = []\n",
    "    flow_frames = []\n",
    "    canvases = []\n",
    "\n",
    "    for frame1, frame2 in tqdm(gen, total=frame_count-2 - skip):\n",
    "        frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(\n",
    "            prev=frame1_gray,\n",
    "            next=frame2_gray,\n",
    "            flow=None,\n",
    "            pyr_scale=0.5,\n",
    "            levels=3,\n",
    "            winsize=winsize,\n",
    "            iterations=3,\n",
    "            poly_n=5,\n",
    "            poly_sigma=1.2,\n",
    "            flags=0\n",
    "        )\n",
    "        \n",
    "        flow_frames.append(Image.fromarray(cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)))\n",
    "        grad_x = flow[..., 0]\n",
    "        grad_y = flow[..., 1]\n",
    "        flow_frames_x.append(Image.fromarray(grad_x))\n",
    "        flow_frames_y.append(Image.fromarray(grad_y))\n",
    "        \n",
    "        hsv = np.zeros_like(frame2)\n",
    "        hsv[..., 1] = 255\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB).astype(np.uint8)\n",
    "\n",
    "        h, w, _ = frame2.shape\n",
    "        canvas = np.zeros((2 * h, w, 3))\n",
    "        canvas[:h, ...] = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "        canvas[h:, ...] = rgb\n",
    "        canvases.append(Image.fromarray(canvas.astype(np.uint8)))\n",
    "    \n",
    "    print('Saving results...')\n",
    "    if not os.path.exists(f'{save_dir}/gradients_{file_name}'):\n",
    "        os.makedirs(f'{save_dir}/gradients_{file_name}')\n",
    "    make_gif(flow_frames, f'{save_dir}/gradients_{file_name}/original.gif', duration=duration)\n",
    "    make_gif(flow_frames_x, f'{save_dir}/gradients_{file_name}/x.gif', duration=duration)\n",
    "    make_gif(flow_frames_y, f'{save_dir}/gradients_{file_name}/y.gif', duration=duration)\n",
    "    make_gif(canvases, f'{save_dir}/gradients_{file_name}/combination.gif', duration=duration)\n",
    "    print('DONE')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfc0ba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c40572b0786437981f235d12bfe551f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "ferneback('./data/shuttle.mp4', './gifs/gradients', skip=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ae551",
   "metadata": {},
   "source": [
    "#### Zadanie 1b\n",
    "Zbadaj wpływ parametru `winsize` na działanie metody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    ('./data/back.mp4', 0),\n",
    "    ('./data/kick.mp4', 0),\n",
    "    ('./data/shot.mp4', 0)\n",
    "]\n",
    "winsizes = [3, 7, 15, 21, 31]\n",
    "for path, skip in paths:\n",
    "    for ws in winsizes:\n",
    "        ferneback(path, f'gifs/winsize_param/size_{ws}', winsize=ws, skip=skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75609860",
   "metadata": {},
   "source": [
    "Komentarz 1:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8771333",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Zadanie 2\n",
    "\n",
    "Powtórz powyższy eksperyment na materiale `shot.mp4` (przewiń materiał do momentu natychmiast po uderzeniu białej bili, ok. 20-25 klatek; framerate wynosi tu ok. 15fps).  \n",
    "*W czym leży trudność? Co jest ograniczeniem metody?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23f33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a842488",
   "metadata": {},
   "source": [
    "Komentarz 2:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0332849",
   "metadata": {},
   "source": [
    "### Zadanie dodatkowe\n",
    "\n",
    "Powróć do takiego przypadku i konfiguracji algorytmu, dla którego uzyskane przez Ciebie wyniki były satysfakcjonujące. Przypomnij sobie zajęcia dotyczące np. segmentacji i wykorzystaj informację o przepływie optycznym do oddzielenia na obrazie obiektów od tła (metoda zupełnie dowolna)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab937e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
